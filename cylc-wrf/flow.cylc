#!Jinja2
[scheduler]
{% set NNS = 1, 2, 4 %}
[task parameters]
    # run multiple instances
    m = 0..4
[scheduling] # Define the tasks and when they should run
  [[graph]]

    R1 = """ # run this graph once
    {% for NN in NNS %}
        run_apptainer{{ NN }}n<m>? => analyse
        run_native{{ NN }}n<m>? => analyse
    {% endfor %}
    """
[runtime] # Define what each task should run
  [[root]]
    [[[environment]]]
      BASE_DIR="/nesi/nobackup/pletzera/coupledModelContainer/run"
      COPY_DIR="/nesi/nobackup/nesi99991/asp23/workspace/pletzera/wrf_exercises/test"
    [[[directives]]]
       --account = nesi99999 # CHANGE
       --hint = nomultithread
       --mem = 20GB
       --time = 03:00:00
       --cpus-per-task = 1
       --ntasks-per-node = 20
       --partition = milan
  {% for NN in NNS %}
  [[run_apptainer{{ NN }}n<m>]]
    platform = mahuika-slurm # Run "cylc conf" to see platforms. 
    execution retry delays = 1*PT10S # retry
    script = """
    rundir="${BASE_DIR}/run_apptainer{{ NN }}n${CYLC_TASK_PARAM_m}"
    mkdir -p $rundir
    cd $rundir
    cp -r $COPY_DIR/* .
    echo "Running using {{ NN }} nodes in $(pwd)... "
    module purge
    module load  Apptainer
    module load intel         ## make sure to use the Intel MPI on the host
    export I_MPI_FABRICS=ofi  ## do no use shm
    srun apptainer exec -B /opt/slurm/lib64/ /nesi/nobackup/pletzera/coupledModelContainer/wrf.sif /software/WRF-4.1.1/main/wrf.exe
    echo "done"
    cd ..
    """
    [[[directives]]] # Default SLURM options for the tasks below
       --nodes = {{ NN }}
  [[run_native{{ NN }}n<m>]]
    platform = mahuika-slurm # Run "cylc conf" to see platforms. 
    execution retry delays = 1*PT10S # retry
    script = """
    rundir="${BASE_DIR}/run_native{{ NN }}n${CYLC_TASK_PARAM_m}"
    mkdir -p $rundir
    cd $rundir
    cp -r $COPY_DIR/* . 
    echo "Running using {{ NN }} nodes in $(pwd)... "
    module purge
    module load HDF5/1.12.2-iimpi-2022a
    WRF_DEPS_DIR=/nesi/nobackup/nesi99991/ASP_winterschool2023/software
    # required for building and running
    export LD_LIBRARY_PATH=$WRF_DEPS_DIR/lib:$LD_LIBRARY_PATH
    srun ./wrf.exe
    echo "done"
    cd ..
    """
    [[[directives]]] # Default SLURM options for the tasks below
       --nodes = {{ NN }}
  {% endfor %}
  [[analyse]]
    platform = localhost
    script = """
    module purge
    module load Python
    cd /nesi/nobackup/pletzera/coupledModelContainer/cylc-wrf
    echo "Running analyse in directory $(pwd)..."
    python analyse.py
    """

